{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try approaches proposed by dataset creators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will look at solution proposed by a dataset creators, I will reuse their preprocessing except for one thing.\n",
    "I'm going to use lemmatization instead of stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/enron_train.txt') as f:\n",
    "    train = f.readlines()\n",
    "trainRaw = [x.strip() for x in train]\n",
    "\n",
    "with open('data/enron_test.txt') as f:\n",
    "    test = f.readlines()\n",
    "testRaw = [x.strip() for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(train):\n",
    "    labels = [i.split('\\t', 1)[0] for i in train]\n",
    "    trainData = [i.split('\\t', 1)[1] for i in train]\n",
    "\n",
    "    ## stemming\n",
    "    from stemming.porter2 import stem\n",
    "    trainData = [\" \".join([lemmatizer.lemmatize(word) for word in sentence.split(\" \")]) for sentence in trainData]\n",
    "\n",
    "    ## replacing http links with $LINK\n",
    "    import re\n",
    "    trainData = [re.sub(r\"(<?)http:\\S+\", \"$LINK\", i) for i in trainData]\n",
    "\n",
    "    ## replcaing money with $MONEY\n",
    "    trainData = [re.sub(r\"\\$\\d+\", \"$MONEY\", i) for i in trainData]\n",
    "\n",
    "    ## replcaing email ids with $EMAILID\n",
    "    trainData = [re.sub(r'[\\w\\.-]+@[\\w\\.-]+', \"$EMAILID\", i) for i in trainData]\n",
    "\n",
    "    ## Lowring the words\n",
    "    trainData = [i.lower() for i in trainData]\n",
    "\n",
    "    ## removing punctuations\n",
    "    import regex as regex\n",
    "    trainData = [regex.sub(r\"[^\\P{P}$]+\", \" \", i) for i in trainData]\n",
    "\n",
    "    ## remove (unnecessary symbols)\n",
    "    trainData = [re.sub(r\"[^0-9A-Za-z/$' ]\", \" \", i) for i in trainData]\n",
    "    \n",
    "    ## replacing Weekdays with $day\n",
    "    regString = r'monday|tuesday|wednesday|thursday|friday|saturday|sunday'\n",
    "    trainData = [re.sub(regString, \"$days\", i) for i in trainData]\n",
    "    \n",
    "    ## replacing Months => $month\n",
    "    regString = r'january|jan|february|feb|march|mar|april|june|july|august|aug|september|sept|october|oct|november|nov|december|dec'\n",
    "    trainData = [re.sub(regString, \"$month\", i) for i in trainData]\n",
    "    \n",
    "    ## after before during => $time\n",
    "    regString = r'after|before|during'\n",
    "    trainData = [re.sub(regString, \"$time\", i) for i in trainData]\n",
    "    \n",
    "    ## replace numbers with $number\n",
    "    trainData = [re.sub(r'\\b\\d+\\b', \"$number\", i) for i in trainData]\n",
    "    \n",
    "    ## me, her, him ,us or them â†’ $me,\n",
    "    trainData = [re.sub(r'\\b(me|her|him|us|them|you)\\b', \"$me\", i) for i in trainData]\n",
    "    \n",
    "    ## striping whitespaces\n",
    "    trainData = [i.strip() for i in trainData]\n",
    "    \n",
    "    return trainData, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(data):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X\n",
    "    \n",
    "def getNgrams(txt, n):\n",
    "    from nltk import ngrams\n",
    "    ngrams = ngrams(txt.split(), n)\n",
    "    l = []\n",
    "    for grams in ngrams:\n",
    "        l.append('_'.join(map(str,grams)))\n",
    "    fl = ' '.join(l)\n",
    "    return fl\n",
    "\n",
    "## Cleaning train data\n",
    "trainData, trainLabels = cleanData(trainRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.DataFrame({\"labels\": trainLabels, \"trainData\": trainData})\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easier output\n",
    "def pred_results(model, x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    print(model)\n",
    "    print(\"train accuracy\", model.score(x_train, y_train))\n",
    "    print(\"test accuracy\", model.score(x_test, y_test))\n",
    "    print()\n",
    "    pred = model.predict(x_test)\n",
    "    print(\"Confusion Matrix\\n\", confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, trainLabels = cleanData(trainRaw)\n",
    "testData, testLabels = cleanData(testRaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how proposed solution works with the lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=2300, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "train accuracy 0.8908308685130412\n",
      "test accuracy 0.7989247311827957\n",
      "\n",
      "Confusion Matrix\n",
      " [[467  76]\n",
      " [111 276]]\n"
     ]
    }
   ],
   "source": [
    "### Adding train and test datasets for SVM as SVM requires \n",
    "### same dimentions, number of features for training and test set\n",
    "data = trainData + testData\n",
    "labels = trainLabels + testLabels\n",
    "\n",
    "## getting TFIDF matrix\n",
    "X = getTFIDF(data)\n",
    "Y = labels\n",
    "\n",
    "### Spliting data 80% train - 20% test\n",
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "  X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "## SVM Model\n",
    "from sklearn.svm import SVC\n",
    "## Value of C is calculated using grid search\n",
    "svm = SVC(C=2300, kernel='rbf')\n",
    "svm.fit(x_train, y_train)\n",
    "\n",
    "pred_results(svm, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get a tiny increase of accuracy\n",
    "```\n",
    "train accuracy: 0.888 vs 0.890 with lemmatizer\n",
    "test accuracy: 0.790 vs 0.798 with lemmatizer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try something else\n",
    "\n",
    "I'm proposing a gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=13, subsample=0.8, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=200, subsample=0.8, random_state=13)\n",
    "gbc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "              presort='auto', random_state=13, subsample=0.8, verbose=0,\n",
      "              warm_start=False)\n",
      "train accuracy 0.8822264049475665\n",
      "test accuracy 0.7989247311827957\n",
      "\n",
      "Confusion Matrix\n",
      " [[466  77]\n",
      " [110 277]]\n"
     ]
    }
   ],
   "source": [
    "pred_results(gbc, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works as good as SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a grid search in order to find the best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {'n_estimators': [100, 150, 180, 200, 220, 250],\n",
    "                     'subsample': [0.8, 0.9, 1],\n",
    "                     'min_samples_split': [2, 3, 4, 6],\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_gbc = GridSearchCV(GradientBoostingClassifier(random_state=13),\n",
    "                               tuned_parameters,\n",
    "                               cv=5, # using 5 folds for cross-validation there\n",
    "                               scoring='accuracy',\n",
    "                               n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=13, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=4,\n",
       "       param_grid={'n_estimators': [100, 150, 180, 200, 220, 250], 'subsample': [0.8, 0.9, 1], 'min_samples_split': [2, 3, 4, 6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_gbc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 3, 'n_estimators': 220, 'subsample': 0.8}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_gbc.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these params on same split we did with SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=3,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=220,\n",
       "              presort='auto', random_state=13, subsample=0.8, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=220, subsample=0.8, min_samples_split=3, random_state=13)\n",
    "gbc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=3,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=220,\n",
      "              presort='auto', random_state=13, subsample=0.8, verbose=0,\n",
      "              warm_start=False)\n",
      "train accuracy 0.8865286367303038\n",
      "test accuracy 0.8032258064516129\n",
      "\n",
      "Confusion Matrix\n",
      " [[468  75]\n",
      " [108 279]]\n"
     ]
    }
   ],
   "source": [
    "pred_results(gbc, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made it above 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data in csv in order to reuse later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(OrderedDict(\n",
    "    {\n",
    "        \"text\": trainData,\n",
    "        \"labels\": trainLabels\n",
    "    }\n",
    "))\n",
    "\n",
    "test_df = pd.DataFrame(OrderedDict(\n",
    "    {\n",
    "        \"text\": testData,\n",
    "        \"labels\": testLabels\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
